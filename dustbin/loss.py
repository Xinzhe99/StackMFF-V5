# -*- coding: utf-8 -*-
# @Author  : XinZhe Xie
# @University  : ZheJiang University

import torch
import torch.nn as nn
import torch.nn.functional as F


class OTLoss(nn.Module):
    """
    ã€å‚ç›´æŸå¤± - æœ‰åºæ¨¡å¼ã€‘
    Optimal Transport Loss using 1D Wasserstein Distance (CDF L1).
    é€‚ç”¨äºæœ‰åºçš„å›¾åƒæ ˆ (Ordered Focal Stack)ã€‚
    """
    def __init__(self):
        super(OTLoss, self).__init__()

    def forward(self, probs, soft_gt):
        """
        Args:
            probs: [B, N, H, W] (Softmaxåçš„æ¦‚ç‡åˆ†å¸ƒ)
            soft_gt: [B, N, H, W] (Soft GT æ¦‚ç‡åˆ†å¸ƒ)
        """
        # 1. è®¡ç®—ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)
        # æ²¿ç€å±‚çº§ç»´åº¦ (dim=1) ç´¯åŠ 
        pred_cdf = torch.cumsum(probs, dim=1)
        gt_cdf = torch.cumsum(soft_gt, dim=1)
        
        # 2. è®¡ç®— Wasserstein è·ç¦» (CDF ä¹‹é—´çš„ L1 è·ç¦»)
        # è¿™æ˜¯ OT Loss çš„æ ¸å¿ƒï¼šåˆ©ç”¨ç‰©ç†è·ç¦»æƒ©ç½šé”™è¯¯
        loss = torch.mean(torch.abs(pred_cdf - gt_cdf))
        return loss


class DivergenceLoss(nn.Module):
    """
    ã€å‚ç›´æŸå¤± - æ— åºæ¨¡å¼ã€‘
    KL Divergence Loss.
    é€‚ç”¨äºä¹±åºçš„å›¾åƒæ ˆ (Unordered/Shuffled Stack) æˆ–æ¶ˆèå®éªŒã€‚
    """
    def __init__(self):
        super(DivergenceLoss, self).__init__()

    def forward(self, logits, soft_gt):
        """
        Args:
            logits: [B, N, H, W] (ç½‘ç»œè¾“å‡ºçš„ Logitsï¼Œæœªç»è¿‡ Softmax)
            soft_gt: [B, N, H, W] (Soft GT æ¦‚ç‡åˆ†å¸ƒ)
        """
        # KL æ•£åº¦è¦æ±‚è¾“å…¥æ˜¯ Log-Probabilities
        log_probs = F.log_softmax(logits, dim=1)
        
        # reduction='batchmean' åœ¨æ•°å­¦ä¸Šæ›´ç¬¦åˆ KL å®šä¹‰
        loss = F.kl_div(log_probs, soft_gt, reduction='batchmean')
        return loss


class SpatialSmoothnessLoss(nn.Module):
    """
    ã€æ°´å¹³æŸå¤± - ç©ºé—´æ­£åˆ™åŒ–ã€‘
    çº¦æŸç›¸é‚»åƒç´ çš„é¢„æµ‹åˆ†å¸ƒåº”ä¿æŒä¸€è‡´æ€§ï¼Œé˜²æ­¢ä¼ªå½±ã€‚
    """
    def __init__(self, mode='ordered'):
        super(SpatialSmoothnessLoss, self).__init__()
        self.mode = mode

    def forward(self, probs):
        """
        Args:
            probs: [B, N, H, W]
        """
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è®¡ç®—ç‰¹å¾
        if self.mode == 'ordered':
            # æœ‰åºæ¨¡å¼ï¼šçº¦æŸ CDF çš„è¿ç»­æ€§ (æ·±åº¦è¿ç»­)
            # ç‰©ç†å«ä¹‰ï¼šç›¸é‚»åƒç´ çš„æ·±åº¦ä¸åº”å‰§çƒˆè·³å˜
            feature = torch.cumsum(probs, dim=1)
        else:
            # æ— åºæ¨¡å¼ï¼šçº¦æŸæ¦‚ç‡å‘é‡çš„ç›¸ä¼¼æ€§ (åˆ†ç±»ä¸€è‡´)
            # ç‰©ç†å«ä¹‰ï¼šç›¸é‚»åƒç´ åº”å±äºåŒä¸€ç±»
            feature = probs

        # è®¡ç®—æ°´å¹³æ–¹å‘æ¢¯åº¦ (Right - Current)
        diff_h = torch.abs(feature[:, :, :, :-1] - feature[:, :, :, 1:])
        
        # è®¡ç®—å‚ç›´æ–¹å‘æ¢¯åº¦ (Down - Current)
        diff_v = torch.abs(feature[:, :, :-1, :] - feature[:, :, 1:, :])

        #æ±‚å¹³å‡
        loss = torch.mean(diff_h) + torch.mean(diff_v)
        return loss


class FusionLoss(nn.Module):
    """
    ã€æ€»æŸå¤±å‡½æ•°å°è£…ã€‘
    ç»Ÿä¸€ç®¡ç†å‚ç›´æŸå¤±(OT/KL)å’Œæ°´å¹³æŸå¤±(Spatial)ã€‚
    """
    def __init__(self, mode='ordered', lambda_spatial=0.01):
        """
        Args:
            mode (str): 'ordered' (ä½¿ç”¨ OT Loss) æˆ– 'unordered' (ä½¿ç”¨ KL Loss)
            lambda_spatial (float): ç©ºé—´æ­£åˆ™åŒ–é¡¹çš„æƒé‡ã€‚
                                    å»ºè®®å€¼: 0.01 ~ 0.1ã€‚è®¾ä¸º 0 åˆ™å…³é—­æ­£åˆ™åŒ–ã€‚
        """
        super(FusionLoss, self).__init__()
        self.mode = mode
        self.lambda_spatial = lambda_spatial
        
        # åˆå§‹åŒ–å­æŸå¤±
        if mode == 'ordered':
            self.main_loss = OTLoss()
            print("ğŸš€ Loss Config: Using [Wasserstein CDF Loss] (Ordered Mode)")
        else:
            self.main_loss = DivergenceLoss()
            print("ğŸ§ª Loss Config: Using [KL Divergence Loss] (Unordered Mode)")
            
        if lambda_spatial > 0:
            self.spatial_loss = SpatialSmoothnessLoss(mode=mode)
            print(f"ğŸŒŠ Loss Config: Spatial Regularization Enabled (lambda={lambda_spatial})")
        else:
            self.spatial_loss = None

    def forward(self, logits, soft_gt):
        """
        Args:
            logits: [B, N, H, W] (ç½‘ç»œç›´æ¥è¾“å‡º)
            soft_gt: [B, N, H, W] (Dataloader åŠ è½½çš„ float32 GT)
        """
        # 1. è®¡ç®—ä¸»æŸå¤± (Fidelity)
        if self.mode == 'ordered':
            # OT Loss éœ€è¦æ¦‚ç‡åˆ†å¸ƒ (Softmaxå)
            probs = F.softmax(logits, dim=1)
            fidelity_loss = self.main_loss(probs, soft_gt)
        else:
            # KL Loss éœ€è¦ logits (å†…éƒ¨åš LogSoftmax)
            # ä¸ºäº†æ¥å£ç»Ÿä¸€ï¼Œè¿™é‡Œç¨å¾®å¤„ç†ä¸€ä¸‹
            fidelity_loss = self.main_loss(logits, soft_gt)
            # å¦‚æœåé¢è¦ç®— spatial lossï¼Œéœ€è¦å…ˆç®—å‡º probs
            if self.spatial_loss is not None:
                probs = F.softmax(logits, dim=1)

        # 2. è®¡ç®—ç©ºé—´æ­£åˆ™åŒ–æŸå¤± (Smoothness)
        smoothness_loss = 0.0
        if self.spatial_loss is not None:
            # æ— è®ºä½•ç§æ¨¡å¼ï¼ŒSpatial Loss éƒ½åŸºäº Probs/CDF è®¡ç®—
            if 'probs' not in locals():
                probs = F.softmax(logits, dim=1)
            smoothness_loss = self.spatial_loss(probs)

        # 3. æ€»æŸå¤±
        total_loss = fidelity_loss + self.lambda_spatial * smoothness_loss
        
        return total_loss